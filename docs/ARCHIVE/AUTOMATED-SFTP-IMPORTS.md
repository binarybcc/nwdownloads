# ğŸš€ Automated SFTP Imports - Architecture & Implementation

**Created:** 2025-12-06
**Updated:** 2025-12-06 (Added Newzware Configuration Reference)
**Status:** Design Complete, Ready for Implementation
**Complexity:** Intermediate
**Estimated Implementation Time:** 2-3 hours
**Newzware Support:** SSH Keys âœ… | Password âœ… | SFTP âœ…

---

## ğŸ“š Table of Contents

1. [Executive Summary](#executive-summary)
2. [Architecture Design](#architecture-design)
3. [Security Model](#security-model)
4. [Newzware Configuration Reference](#newzware-configuration-reference)
5. [Implementation Guide](#implementation-guide)
6. [Monitoring & Maintenance](#monitoring--maintenance)
7. [Troubleshooting](#troubleshooting)
8. [Future Enhancements](#future-enhancements)
9. [Senior Developer Notes](#senior-developer-notes)

---

## Executive Summary

### The Problem

**Current State (Manual):**
- User downloads CSV from Newzware manually
- User uploads CSV via web interface manually
- Dashboard requires human intervention for updates
- Weekend/holiday updates missed
- Prone to human error (forgot to update, uploaded wrong file)

**Pain Points:**
- Time-consuming (15 minutes per week)
- Not scalable (what if we need daily updates?)
- No audit trail of what was uploaded when
- Dashboard can become stale

### The Solution

**Automated SFTP Pipeline:**
- Newzware generates reports on schedule (daily/weekly)
- Newzware auto-uploads via SFTP to isolated Docker container
- Cron job processes files automatically
- Dashboard always current with zero human intervention
- Full audit trail and error handling

**Benefits:**
- âœ… Zero manual work after setup
- âœ… Always up-to-date data
- âœ… Runs on weekends/holidays
- âœ… Handles multiple report types
- âœ… Error detection and logging
- âœ… Scalable to any frequency

### Key Design Decisions

**1. Docker-Isolated SFTP Server (Not Direct NAS SSH)**

**Why:** Defense in depth security

```
âŒ Bad: Internet â†’ NAS SSH (Port 22) â†’ Full filesystem access
âœ… Good: Internet â†’ Docker SFTP (Port 2222) â†’ Hotfolder only
```

**Reasoning:**
- Production NAS hosts critical business infrastructure (news, video, etc.)
- SSH compromise = entire business data at risk
- Docker isolation = compromise limited to empty hotfolder
- Can rebuild container in minutes if needed
- Principle: Minimize blast radius

**2. SSH Key Authentication (Recommended) or Password Fallback**

**Why:** Newzware DOES support SSH key authentication via `privatekey` parameter

**Primary Option - SSH Keys (Most Secure):**
- Newzware `privatekey` parameter points to local SSH private key
- No password transmitted over network
- Key rotation possible without Newzware config changes
- Industry best practice for automated systems

**Fallback Option - Password Authentication:**
- Use if SSH key setup encounters issues
- Strong password (20+ chars, complex)
- Still secure with proper layered defenses

**Layered Security (Applied to Both Methods):**
- Port forwarding on non-standard port (2222, not 22)
- Optional: IP whitelisting if Newzware has static IP
- Optional: Fail2ban for brute force protection
- Logging all access attempts

**Recommendation:** Start with SSH keys, fall back to password only if needed

**3. Cron-Based Processing (Not Real-Time File Watcher)**

**Why:** Simpler, matches update frequency

**Reasoning:**
- Reports arrive once per day (or week) on schedule
- No need for real-time processing (15-min delay acceptable)
- Cron is battle-tested, well-understood, easy to debug
- File watcher adds complexity (daemon process, error handling)
- YAGNI principle: Don't build features you don't need

**When to reconsider:** If requirements change to real-time processing

**4. Separate Processing Logic (Not Coupled to Web Upload)**

**Why:** Reusability and maintainability

**Code Structure:**
```
upload.php (web interface)
    â†“ calls
process_all_subscriber_import.php (shared logic)
    â†‘ called by
process_hotfolder.php (CLI automation)
```

**Benefits:**
- Single source of truth for import logic
- Test once, works everywhere
- Easy to add new entry points (API, CLI tools)
- DRY principle (Don't Repeat Yourself)

---

## Architecture Design

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INTERNET                              â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  Newzware    â”‚  Scheduled Export (5 AM daily)            â”‚
â”‚  â”‚   Server     â”‚  AllSubscriberReport_20251206.csv         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚ SFTP (Port 2222)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   UBIQUITI ROUTER                            â”‚
â”‚                                                              â”‚
â”‚  Port Forward Rule:                                          â”‚
â”‚  External 2222 â†’ 192.168.1.254:2222                         â”‚
â”‚                                                              â”‚
â”‚  Optional: IP Whitelist (Newzware source IP)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SYNOLOGY NAS (192.168.1.254)                    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           DOCKER NETWORK                             â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  SFTP Containerâ”‚         â”‚  Web Container   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  (Port 2222)   â”‚         â”‚  (PHP + Apache)  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                â”‚         â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  User: newzwareâ”‚         â”‚  Cron Job:       â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Pass: ******  â”‚         â”‚  */15 * * * *    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                â”‚         â”‚  process_hotfolderâ”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚          â”‚                           â”‚              â”‚   â”‚
â”‚  â”‚          â”‚ Volume Mount              â”‚ Volume Mount â”‚   â”‚
â”‚  â”‚          â–¼                           â–¼              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚         HOTFOLDER                            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                                              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  /incoming   - Files arrive here            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  /processed  - Successful imports           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  /errors     - Failed imports               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  /archive    - Long-term storage            â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                           â”‚                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                             â”‚
â”‚                              â–¼                             â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚                  â”‚  MariaDB Database  â”‚                    â”‚
â”‚                  â”‚                    â”‚                    â”‚
â”‚                  â”‚  - daily_snapshots â”‚                    â”‚
â”‚                  â”‚  - subscriber_     â”‚                    â”‚
â”‚                  â”‚    snapshots       â”‚                    â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

**1. Scheduled Export (Newzware â†’ SFTP Container)**
```
Time: 5:00 AM daily
Action: Newzware generates AllSubscriberReport CSV
Destination: SFTP upload to cdash.upstatetoday.com:2222
Path: /incoming/AllSubscriberReport_20251206_050000.csv
```

**2. File Arrival (SFTP Container)**
```
Location: /home/newzware/incoming/ (inside container)
Maps to: ./hotfolder/incoming/ (on NAS)
Permissions: 755 (newzware user can write)
```

**3. Automated Processing (Cron Job)**
```
Schedule: Every 15 minutes (*/15 * * * *)
Script: php /var/www/html/cli/process_hotfolder.php
Action:
  1. Scan /hotfolder/incoming for CSV files
  2. Detect file type by filename pattern
  3. Process file (import to database)
  4. Move to /processed or /errors based on result
  5. Log outcome
```

**4. Database Update**
```
Tables Updated:
  - daily_snapshots (aggregate counts)
  - subscriber_snapshots (individual records)
Action: UPSERT (update if exists, insert if new)
Result: Dashboard auto-refreshes with new data
```

### File Processing Logic

**Filename Pattern Detection:**
```php
/AllSubscriber/i      â†’ processAllSubscriberReport()
/Vacation/i           â†’ processVacationReport()
/Expiring/i           â†’ processExpiringReport()
/Unknown pattern/     â†’ Move to errors folder
```

**Error Handling Strategy:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV File   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ File Validation â”‚
â”‚ - Size check    â”‚â”€â”€â”€â”€ Fail â”€â”€â†’ /errors/filename.csv.invalid
â”‚ - Format check  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Pass
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Import     â”‚
â”‚ - Parse CSV     â”‚
â”‚ - Validate data â”‚â”€â”€â”€â”€ Fail â”€â”€â†’ /errors/filename.csv.error
â”‚ - Insert DB     â”‚              + error.log
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Success
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  /processed/    â”‚
â”‚ filename_       â”‚
â”‚ 20251206.csv    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this approach:**
- Failed files preserved for debugging
- Clear separation: success vs. failure
- Timestamps prevent filename collisions
- Error logs provide context for troubleshooting

---

## Security Model

### Defense in Depth Strategy

**Layer 1: Network Perimeter (Firewall)**
```
Ubiquiti Router:
  - Port forwarding: Only 2222 (non-standard)
  - Optional: IP whitelist (if Newzware has static IP)
  - DDoS protection: Rate limiting
  - Logging: All connection attempts

Benefit: First line of defense
Risk mitigation: Block 99% of automated attacks
```

**Layer 2: Docker Isolation**
```
SFTP Container:
  - No access to NAS filesystem
  - Only hotfolder volume mounted
  - Runs as non-root user (UID 1000)
  - Can be destroyed/rebuilt instantly

Benefit: Compromise doesn't spread
Risk mitigation: Even if hacked, attacker sees empty folder
```

**Layer 3: Filesystem Restrictions**
```
SFTP User Permissions:
  - Home: /home/newzware/incoming (chroot jail)
  - Write: incoming folder only
  - Read: Same (can't traverse to other directories)
  - No shell access (SFTP only, not SSH)

Benefit: Principle of least privilege
Risk mitigation: Can't execute commands, only transfer files
```

**Layer 4: Authentication & Access Control**
```
Strong Password:
  - Minimum 20 characters
  - Mixed case, numbers, symbols
  - Stored in .env (not in code)
  - Rotate every 90 days

Benefit: Hard to brute force
Risk mitigation: 20-char password = 10^35 combinations
```

**Layer 5: Monitoring & Logging**
```
All access logged:
  - Successful logins (timestamp, IP)
  - Failed attempts (for alerting)
  - File transfers (what, when, size)
  - Processing results (success/error)

Benefit: Detective control (know if breach occurs)
Risk mitigation: Can identify and respond to incidents
```

### Security Comparison

| Attack Vector | Docker SFTP | Direct NAS SSH | Mitigation |
|---------------|-------------|----------------|------------|
| Brute force password | ğŸŸ¡ Medium risk | ğŸ”´ High risk | Fail2ban, strong password |
| Compromised credentials | ğŸŸ¢ Low impact | ğŸ”´ Critical impact | Isolation limits damage |
| Zero-day exploit | ğŸŸ¢ Container only | ğŸ”´ Full NAS | Rebuild container |
| Insider threat | ğŸŸ¡ Limited access | ğŸ”´ Full access | Audit logs |
| DDoS attack | ğŸŸ¢ Container restarts | ğŸŸ¡ NAS performance | Rate limiting |

### Attack Scenario Analysis

**Scenario 1: Password Compromised**

**Docker SFTP Approach:**
```
1. Attacker gets password
2. Logs into SFTP container
3. Sees: /home/newzware/incoming (empty folder)
4. Can upload files (processed by script with validation)
5. Can download processed reports (if any)

Impact: ğŸŸ¢ Low
  - No NAS access
  - No business data accessible
  - Can't execute commands
  - Worst case: Upload malicious CSV (caught by validation)

Response:
  - Rotate password (1 minute)
  - Rebuild container (2 minutes)
  - Review logs for suspicious activity
  - Check processed files for anomalies
```

**Direct NAS SSH Approach:**
```
1. Attacker gets password
2. Logs into NAS via SSH
3. Sees: Entire business filesystem (news, video, accounting)
4. Can download confidential data
5. Can modify/delete files
6. Can install malware

Impact: ğŸ”´ Critical
  - Full business compromise
  - Data exfiltration
  - Potential ransomware
  - Regulatory violations (data breach)

Response:
  - Incident response team
  - Forensic analysis (days)
  - Customer notification
  - Potential lawsuits
```

**Conclusion:** Docker isolation reduces critical incident to minor inconvenience.

---

## Newzware Configuration Reference

**Source:** Newzware Release Notes - Report Scheduler Output Format Options and Scripting

### Overview

Newzware's Report Scheduler supports automated SFTP uploads using the `DefaultSchedulerScript`. This built-in script handles FTP/SFTP transfers, file naming with timestamps, compression, and more.

### Configuration Format

**Script Structure:**
```
com.icanon.report.DefaultSchedulerScript,parameter1=value1;parameter2=value2;parameter3=value3;
```

**Key Points:**
- Script name and parameters separated by **comma**
- Parameters are **semicolon-delimited** name=value pairs
- No spaces around equals signs or semicolons
- Case-sensitive parameter names

### Required Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| `protocol` | `sftp` | Use SFTP instead of FTP |
| `server` | IP or hostname | Target SFTP server (e.g., `192.168.1.254` or `cdash.upstatetoday.com`) |
| `user` | Username | SFTP username (e.g., `newzware`) |
| `password` | Password | User password (if not using SSH key) |

### SSH Key Authentication (Recommended)

**Parameter:** `privatekey`

**Example:**
```
privatekey=C:/Newzware/config/sftp/newzware_sftp_key
```

**Notes:**
- Path to **private key file** on Newzware server
- Use forward slashes (`/`) even on Windows
- Must be accessible to Newzware application
- Public key must be in `~/.ssh/authorized_keys` on SFTP server

### Optional Parameters (Commonly Used)

| Parameter | Values | Description |
|-----------|--------|-------------|
| `port` | Integer | TCP port (default: 22, we use: 2222) |
| `folder` | Path | Target directory on SFTP server (e.g., `/incoming`) |
| `outputfile` | Filename | Rename file on server (supports `<datestamp>` keyword) |
| `skipempty` | `true`/`false` | Don't transfer zero-byte files |
| `gzip` | `true`/`false` | Compress with GZIP (.gz suffix) |
| `zip` | `true`/`false` | Compress with ZIP (.zip suffix) |

### Datestamp Keywords

**Available Keywords:**

| Keyword | Format | Example Output |
|---------|--------|----------------|
| `<datestamp>` | YYYYMMDD | `20251206` |
| `<timestamp>` | YYYYMMDDhhmmss | `20251206143022` |
| `<datestamp!EEE!>` | Day of week | `Fri` |
| `<datestamp!yyyy-MM-dd!>` | Custom format | `2025-12-06` |

**Date Math (Add/Subtract Days):**
- `<datestamp!YYYYMMDD(-7d)!>` = 7 days ago
- `<datestamp!YYYYMMDD(+1d)!>` = Tomorrow

**Example Filename:**
```
outputfile=AllSubscriberReport_<datestamp>.csv
# Result: AllSubscriberReport_20251206.csv
```

### Example Configurations

#### Option 1: SSH Key Authentication (Recommended)

**For Production (Public IP/Domain):**
```
com.icanon.report.DefaultSchedulerScript,protocol=sftp;server=cdash.upstatetoday.com;port=2222;user=newzware;privatekey=C:/Newzware/config/sftp/newzware_sftp_key;folder=/incoming;outputfile=AllSubscriberReport_<datestamp>.csv;skipempty=true;
```

**For Local Testing (Direct IP):**
```
com.icanon.report.DefaultSchedulerScript,protocol=sftp;server=192.168.1.254;port=2222;user=newzware;privatekey=C:/Newzware/config/sftp/newzware_sftp_key;folder=/incoming;outputfile=AllSubscriberReport_<datestamp>.csv;skipempty=true;
```

#### Option 2: Password Authentication (Fallback)

**For Production:**
```
com.icanon.report.DefaultSchedulerScript,protocol=sftp;server=cdash.upstatetoday.com;port=2222;user=newzware;password=YourSecurePassword123!;folder=/incoming;outputfile=AllSubscriberReport_<datestamp>.csv;skipempty=true;
```

#### Option 3: With Compression (For Large Files)

**GZIP Compression:**
```
com.icanon.report.DefaultSchedulerScript,protocol=sftp;server=cdash.upstatetoday.com;port=2222;user=newzware;privatekey=C:/Newzware/config/sftp/newzware_sftp_key;folder=/incoming;outputfile=AllSubscriberReport_<datestamp>.csv;skipempty=true;gzip=true;
```

**Result:** File uploaded as `AllSubscriberReport_20251206.csv.gz`

#### Option 4: Multiple Reports (Different Folders)

**AllSubscriberReport to /incoming:**
```
com.icanon.report.DefaultSchedulerScript,protocol=sftp;server=cdash.upstatetoday.com;port=2222;user=newzware;privatekey=C:/Newzware/config/sftp/newzware_sftp_key;folder=/incoming;outputfile=AllSubscriberReport_<datestamp>.csv;skipempty=true;
```

**VacationReport to /incoming (same folder, different name):**
```
com.icanon.report.DefaultSchedulerScript,protocol=sftp;server=cdash.upstatetoday.com;port=2222;user=newzware;privatekey=C:/Newzware/config/sftp/newzware_sftp_key;folder=/incoming;outputfile=VacationReport_<datestamp>.csv;skipempty=true;
```

### Newzware Report Scheduler Setup

**Step-by-Step Configuration:**

1. **Open Newzware Report Scheduler**
   - Navigate to: Reports â†’ Report Macro Automatic Scheduler

2. **Create/Edit Scheduled Report**
   - Report: Select "All Subscriber Report" from query list
   - Output Format: Choose `CSV` or `PLAIN TEXT`
   - Schedule: Set frequency (Daily, Weekly, etc.)

3. **Configure Output Script**
   - **Field:** "Output Script"
   - **Value:** Paste one of the example configurations above
   - **Timing:** Can configure "before" and "after" scripts

4. **Test Configuration**
   - Click "Test Report Schedule" button
   - Verify file appears in SFTP server `/incoming` folder
   - Check Newzware logs for any errors

5. **Activate Schedule**
   - Enable checkbox "Schedule Active"
   - Save configuration

### Testing Newzware SFTP Connection

**Before scheduling, test the connection:**

**Method 1: Manual SFTP Test from Newzware Server**
```bash
# On Windows Newzware server
sftp -P 2222 newzware@cdash.upstatetoday.com

# Try uploading a test file
put C:\temp\test.csv /incoming/test.csv

# Verify it appears
ls /incoming/

# Clean up
rm /incoming/test.csv
exit
```

**Method 2: Use Newzware Test Feature**
- Create a simple test report (e.g., 10 rows)
- Configure with SFTP script
- Click "Run Now" button
- Check SFTP server for file arrival

### Common Newzware Configuration Issues

**Issue: "Connection refused"**
- **Cause:** Port forwarding not configured or firewall blocking
- **Fix:** Verify router port forward rule (External 2222 â†’ NAS 2222)

**Issue: "Authentication failed"**
- **Cause:** Wrong username, password, or SSH key path
- **Fix:** Double-check credentials, verify SSH key file exists at specified path

**Issue: "Permission denied" when uploading**
- **Cause:** SFTP user doesn't have write access to folder
- **Fix:** Verify SFTP container configuration allows writes to `/incoming`

**Issue: File appears but processing doesn't run**
- **Cause:** Cron job not configured or process_hotfolder.php has errors
- **Fix:** Check cron logs, test process_hotfolder.php manually

**Issue: `<datestamp>` appears literally in filename**
- **Cause:** Wrong syntax or unsupported keyword
- **Fix:** Use exact syntax `<datestamp>` (case-sensitive), verify Newzware version supports keywords

### Security Best Practices

**SSH Key Security:**
1. **Generate dedicated key pair** (not shared with other systems)
2. **Use passphrase-protected keys** (adds extra security layer)
3. **Restrict key permissions** on Newzware server:
   ```bash
   # On Newzware server
   chmod 600 /path/to/newzware_sftp_key
   ```
4. **Regular key rotation** (every 6-12 months)

**Password Security (If Used):**
1. **20+ character password** with mixed case, numbers, symbols
2. **Unique password** (not reused from other systems)
3. **Store in Newzware config** (not in documentation)
4. **Change every 90 days** (security best practice)

**Network Security:**
1. **Non-standard port** (2222 instead of 22)
2. **IP whitelisting** if Newzware has static IP
3. **Monitor logs** for unauthorized access attempts
4. **Fail2ban** to block brute force attacks

---

## Implementation Guide

### Phase 1: Local Development Setup

**Objective:** Test SFTP container and file processing locally before production deployment.

#### Step 1.1: Add SFTP Service to docker-compose.yml

**File:** `docker-compose.yml`

```yaml
services:
  # Existing services (database, web) ...

  # ============================================================
  # SFTP Server for Automated Imports
  # ============================================================
  #
  # Security Notes:
  # - Isolated from NAS filesystem (only hotfolder visible)
  # - Runs as non-root user (UID 1000)
  # - No shell access (SFTP only)
  # - Strong password required
  #
  # Port 2222: Non-standard port for security (not 22)
  #
  sftp:
    image: atmoz/sftp:latest
    container_name: circulation_sftp
    restart: unless-stopped

    # Network Configuration
    ports:
      - "2222:22"  # External 2222 â†’ Internal SSH 22

    networks:
      - circulation_network

    # Filesystem Access (RESTRICTED)
    volumes:
      # ONLY hotfolder/incoming is accessible
      # SFTP user cannot see NAS filesystem
      - ./hotfolder/incoming:/home/newzware/incoming

    # User Configuration
    # Format: username:password:uid:gid:directory
    #
    # username: newzware (SFTP login name)
    # password: Use environment variable (don't hardcode!)
    # uid:gid: 1000:1000 (matches Docker user for permissions)
    # directory: incoming (home directory, chroot jail)
    #
    environment:
      SFTP_USERS: "newzware:${SFTP_PASSWORD:-DevelopmentPassword123!}:1000:1000:incoming"

    # Health Monitoring
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep -v grep | grep sshd"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

    # Security: Drop unnecessary capabilities
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - FOWNER
```

**Why atmoz/sftp image?**
- Well-maintained (updated regularly)
- Minimal attack surface (Alpine Linux base)
- Simple configuration
- 10M+ pulls (battle-tested)
- Active community support

#### Step 1.2: Create Hotfolder Structure

```bash
cd $PROJECT_ROOT

# Create directory structure
mkdir -p hotfolder/{incoming,processed,errors,archive}

# Set permissions (important for Docker UID 1000)
chmod -R 755 hotfolder

# Create .gitkeep to preserve empty directories
touch hotfolder/incoming/.gitkeep
touch hotfolder/processed/.gitkeep
touch hotfolder/errors/.gitkeep
touch hotfolder/archive/.gitkeep
```

**Add to .gitignore:**
```
# Hotfolder files (but keep directory structure)
hotfolder/incoming/*
hotfolder/processed/*
hotfolder/errors/*
hotfolder/archive/*

# Keep .gitkeep files
!hotfolder/**/.gitkeep
```

**Why this structure:**
- `incoming/`: Files arrive here (SFTP upload target)
- `processed/`: Successful imports (audit trail)
- `errors/`: Failed imports (debugging)
- `archive/`: Long-term storage (move from processed monthly)

#### Step 1.3: Start SFTP Container

```bash
# Start all services
docker compose up -d

# Verify SFTP is running
docker compose ps

# Expected output:
# circulation_sftp   atmoz/sftp:latest   Up X seconds (healthy)   0.0.0.0:2222->22/tcp

# Check logs
docker logs circulation_sftp

# Expected output:
# [INFO] Creating user: newzware
# [INFO] Home directory: /home/newzware/incoming
# [INFO] Starting SSH daemon...
```

#### Step 1.4: Test SFTP Access

**Test 1: Connection**
```bash
# Connect via SFTP
sftp -P 2222 newzware@localhost

# Password: DevelopmentPassword123!

# Expected: Successful login
# You should see: sftp>
```

**Test 2: Directory Listing**
```
sftp> pwd
/home/newzware/incoming

sftp> ls
# Should be empty (or just .gitkeep)

sftp> ls ..
# Should fail (permission denied - can't traverse up)
# This is GOOD - means chroot is working!
```

**Test 3: File Upload**
```
sftp> put test.csv
# Create a test.csv locally first: echo "test" > test.csv

Uploading test.csv to /home/newzware/incoming/test.csv
test.csv                              100%    5     0.1KB/s   00:00

sftp> ls
test.csv

sftp> exit
```

**Test 4: Verify File Arrival**
```bash
ls -lh hotfolder/incoming/

# Expected output:
# -rw-r--r-- 1 user staff 5B Dec 6 14:00 test.csv

# File is there! SFTP â†’ Hotfolder mapping works.
```

**Test 5: Security - Try SSH Access (Should Fail)**
```bash
ssh -p 2222 newzware@localhost

# Expected: Permission denied or connection closes
# This is GOOD - means SSH is disabled, only SFTP works
```

---

### Phase 2: File Processing Script

**Objective:** Automate detection and processing of uploaded CSV files.

#### Step 2.1: Create CLI Directory Structure

```bash
mkdir -p web/cli
```

**Why separate CLI directory:**
- Organized: Web code vs. background scripts
- Security: Can restrict web access to this folder
- Clarity: Easy to find automation scripts

#### Step 2.2: Create Shared Import Logic

**File:** `web/lib/ImportProcessor.php`

```php
<?php
/**
 * CSV Import Processing Library
 *
 * Shared logic for importing CSV files from various sources:
 * - Web upload (upload.php)
 * - SFTP automation (process_hotfolder.php)
 * - CLI manual imports
 *
 * Design Pattern: Single Responsibility
 * This class ONLY handles CSV processing logic.
 * It doesn't care WHERE the file came from.
 *
 * Author: Automated SFTP System
 * Date: 2025-12-06
 */

class ImportProcessor {
    private $pdo;
    private $logger;

    public function __construct($pdo, $logger = null) {
        $this->pdo = $pdo;
        $this->logger = $logger ?? new FileLogger('/var/log/imports.log');
    }

    /**
     * Process AllSubscriber CSV report
     *
     * @param string $filePath Absolute path to CSV file
     * @return array ['success' => bool, 'message' => string, 'stats' => array]
     *
     * @throws InvalidArgumentException If file doesn't exist
     * @throws RuntimeException If database errors occur
     */
    public function processAllSubscriberReport($filePath) {
        // Validate file exists
        if (!file_exists($filePath)) {
            throw new InvalidArgumentException("File not found: $filePath");
        }

        // Validate file size (reasonable limits)
        $fileSize = filesize($filePath);
        if ($fileSize < 100) {
            throw new RuntimeException("File too small (likely corrupted): $fileSize bytes");
        }
        if ($fileSize > 50 * 1024 * 1024) { // 50 MB
            throw new RuntimeException("File too large (possible attack): $fileSize bytes");
        }

        $this->logger->info("Processing AllSubscriber report: " . basename($filePath));

        // Import existing logic from upload.php
        // (Copy your current CSV processing code here)
        //
        // Key steps:
        // 1. Open CSV file
        // 2. Validate headers
        // 3. Process rows
        // 4. UPSERT to daily_snapshots
        // 5. UPSERT to subscriber_snapshots
        // 6. Return statistics

        $stats = [
            'file' => basename($filePath),
            'size' => $fileSize,
            'rows_processed' => 0,
            'rows_inserted' => 0,
            'rows_updated' => 0,
            'errors' => [],
            'processing_time' => 0,
            'timestamp' => date('Y-m-d H:i:s')
        ];

        $startTime = microtime(true);

        try {
            // TODO: Move your CSV processing logic here
            // For now, return placeholder

            $stats['processing_time'] = microtime(true) - $startTime;

            $this->logger->info("Import successful", $stats);

            return [
                'success' => true,
                'message' => 'Import completed successfully',
                'stats' => $stats
            ];

        } catch (Exception $e) {
            $this->logger->error("Import failed: " . $e->getMessage());

            return [
                'success' => false,
                'message' => $e->getMessage(),
                'stats' => $stats
            ];
        }
    }

    /**
     * Process Vacation report (future enhancement)
     */
    public function processVacationReport($filePath) {
        // TODO: Implement vacation report processing
        $this->logger->info("Vacation report processing not yet implemented");

        return [
            'success' => false,
            'message' => 'Vacation report processing coming soon',
            'stats' => []
        ];
    }
}

/**
 * Simple file-based logger
 */
class FileLogger {
    private $logFile;

    public function __construct($logFile) {
        $this->logFile = $logFile;

        // Ensure log directory exists
        $dir = dirname($logFile);
        if (!is_dir($dir)) {
            mkdir($dir, 0755, true);
        }
    }

    public function info($message, $context = []) {
        $this->log('INFO', $message, $context);
    }

    public function error($message, $context = []) {
        $this->log('ERROR', $message, $context);
    }

    private function log($level, $message, $context) {
        $timestamp = date('Y-m-d H:i:s');
        $contextStr = !empty($context) ? ' ' . json_encode($context) : '';
        $line = "[$timestamp] $level: $message$contextStr\n";

        file_put_contents($this->logFile, $line, FILE_APPEND);
    }
}
?>
```

**Why this design:**
- **Reusable**: Same logic for web and CLI
- **Testable**: Can unit test without web server
- **Maintainable**: Fix bug once, works everywhere
- **SOLID Principles**: Single Responsibility (only processes CSVs)

#### Step 2.3: Create Hotfolder Processing Script

**File:** `web/cli/process_hotfolder.php`

```php
#!/usr/bin/env php
<?php
/**
 * Hotfolder Processor
 *
 * Automatically detects and processes CSV files uploaded via SFTP.
 * Runs via cron every 15 minutes.
 *
 * Usage:
 *   php process_hotfolder.php
 *
 * Cron:
 *   */15 * * * * php /var/www/html/cli/process_hotfolder.php >> /var/log/hotfolder.log 2>&1
 *
 * Exit Codes:
 *   0 - Success (no files or all processed successfully)
 *   1 - Partial failure (some files failed)
 *   2 - Critical error (script error, not file error)
 *
 * Design Philosophy:
 *   - Fail gracefully (one bad file doesn't stop others)
 *   - Log everything (for debugging and audit)
 *   - Move files (don't delete - preserve for analysis)
 *   - Idempotent (safe to run multiple times)
 *
 * @author Automated SFTP System
 * @date 2025-12-06
 */

// Ensure running from CLI, not web
if (php_sapi_name() !== 'cli') {
    die('This script must be run from command line');
}

// Configuration
require_once __DIR__ . '/../config.php';
require_once __DIR__ . '/../lib/ImportProcessor.php';

// Paths
$hotfolder = '/var/www/hotfolder';
$incoming  = $hotfolder . '/incoming';
$processed = $hotfolder . '/processed';
$errors    = $hotfolder . '/errors';

// Database connection
try {
    $pdo = new PDO(
        "mysql:host=" . getenv('DB_HOST') . ";dbname=" . getenv('DB_NAME'),
        getenv('DB_USER'),
        getenv('DB_PASSWORD')
    );
    $pdo->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
} catch (PDOException $e) {
    echo "[ERROR] Database connection failed: " . $e->getMessage() . "\n";
    exit(2);
}

// Initialize processor
$processor = new ImportProcessor($pdo);

// Scan for CSV files
$files = glob($incoming . '/*.csv');

if (empty($files)) {
    echo date('Y-m-d H:i:s') . " [INFO] No files to process\n";
    exit(0);
}

echo date('Y-m-d H:i:s') . " [INFO] Found " . count($files) . " file(s)\n";

$successCount = 0;
$errorCount = 0;

foreach ($files as $file) {
    $filename = basename($file);
    $timestamp = date('Ymd_His');

    echo "\n" . date('Y-m-d H:i:s') . " [PROCESSING] $filename\n";

    try {
        // Detect file type by filename pattern
        if (preg_match('/AllSubscriber/i', $filename)) {
            // Process AllSubscriber report
            $result = $processor->processAllSubscriberReport($file);

            if ($result['success']) {
                // Move to processed folder with timestamp
                $newPath = $processed . '/' . pathinfo($filename, PATHINFO_FILENAME)
                         . '_' . $timestamp . '.csv';
                rename($file, $newPath);

                echo date('Y-m-d H:i:s') . " [SUCCESS] AllSubscriber report processed\n";
                echo "  Rows: " . $result['stats']['rows_processed'] . "\n";
                echo "  Time: " . round($result['stats']['processing_time'], 2) . "s\n";
                echo "  Moved to: " . basename($newPath) . "\n";

                $successCount++;
            } else {
                // Move to errors folder
                $errorPath = $errors . '/' . $filename . '.error';
                rename($file, $errorPath);

                // Save error details
                file_put_contents(
                    $errorPath . '.log',
                    date('Y-m-d H:i:s') . " ERROR: " . $result['message'] . "\n" .
                    json_encode($result['stats'], JSON_PRETTY_PRINT)
                );

                echo date('Y-m-d H:i:s') . " [ERROR] Processing failed: " . $result['message'] . "\n";
                $errorCount++;
            }
        }
        elseif (preg_match('/Vacation/i', $filename)) {
            // Process Vacation report
            $result = $processor->processVacationReport($file);

            if ($result['success']) {
                $newPath = $processed . '/' . pathinfo($filename, PATHINFO_FILENAME)
                         . '_' . $timestamp . '.csv';
                rename($file, $newPath);
                echo date('Y-m-d H:i:s') . " [SUCCESS] Vacation report processed\n";
                $successCount++;
            } else {
                echo date('Y-m-d H:i:s') . " [INFO] Vacation processing not yet implemented\n";
                // Keep file in incoming for manual processing
            }
        }
        else {
            // Unknown file type
            echo date('Y-m-d H:i:s') . " [WARNING] Unknown file type: $filename\n";

            // Move to errors folder for manual review
            $unknownPath = $errors . '/unknown_' . $timestamp . '_' . $filename;
            rename($file, $unknownPath);

            file_put_contents(
                $unknownPath . '.log',
                "Unknown file type - unable to determine processing method\n" .
                "Filename pattern didn't match any known report types\n" .
                "Expected patterns: AllSubscriber*, Vacation*\n"
            );

            $errorCount++;
        }

    } catch (Exception $e) {
        // Unexpected error during processing
        echo date('Y-m-d H:i:s') . " [ERROR] Exception: " . $e->getMessage() . "\n";

        // Move to errors
        $errorPath = $errors . '/exception_' . $timestamp . '_' . $filename;
        rename($file, $errorPath);

        file_put_contents(
            $errorPath . '.log',
            date('Y-m-d H:i:s') . " EXCEPTION\n" .
            "Message: " . $e->getMessage() . "\n" .
            "File: " . $e->getFile() . "\n" .
            "Line: " . $e->getLine() . "\n" .
            "Trace:\n" . $e->getTraceAsString()
        );

        $errorCount++;
    }
}

// Summary
echo "\n" . str_repeat('=', 60) . "\n";
echo date('Y-m-d H:i:s') . " [SUMMARY]\n";
echo "  Total files: " . count($files) . "\n";
echo "  Successful: $successCount\n";
echo "  Failed: $errorCount\n";
echo str_repeat('=', 60) . "\n";

// Exit code based on results
if ($errorCount > 0 && $successCount === 0) {
    exit(1); // All failed
} elseif ($errorCount > 0) {
    exit(1); // Some failed
} else {
    exit(0); // All succeeded
}
?>
```

**Make executable:**
```bash
chmod +x web/cli/process_hotfolder.php
```

**Test manually:**
```bash
# Copy a test CSV to incoming
cp test_data/AllSubscriberReport.csv hotfolder/incoming/

# Run processor
docker exec circulation_web php /var/www/html/cli/process_hotfolder.php

# Expected output:
# [INFO] Found 1 file(s)
# [PROCESSING] AllSubscriberReport.csv
# [SUCCESS] AllSubscriber report processed
#   Rows: 8000
#   Time: 2.5s
#   Moved to: AllSubscriberReport_20251206_140000.csv
# [SUMMARY]
#   Total files: 1
#   Successful: 1
#   Failed: 0
```

---

### Phase 3: Automated Processing (Cron Job)

**Objective:** Run hotfolder processor every 15 minutes automatically.

#### Step 3.1: Add Cron to Docker Container

**Update Dockerfile:**

```dockerfile
FROM php:8.2-apache

# ... existing packages ...

# Install cron for scheduled tasks
RUN apt-get update && apt-get install -y \
    cron \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy application code
COPY ./web /var/www/html

# Copy crontab file
COPY docker/crontab /etc/cron.d/hotfolder-cron

# Set permissions on crontab
RUN chmod 0644 /etc/cron.d/hotfolder-cron

# Install crontab
RUN crontab /etc/cron.d/hotfolder-cron

# Create log directory
RUN mkdir -p /var/log/hotfolder && chmod 755 /var/log/hotfolder

# Start cron daemon and Apache
# (Both must run in foreground for Docker)
CMD cron && apache2-foreground
```

**Create crontab file:**

**File:** `docker/crontab`

```cron
# Hotfolder Processing Cron Jobs
# Runs automated CSV imports from SFTP uploads

# Process hotfolder every 15 minutes
# Logs to /var/log/hotfolder/processing.log
*/15 * * * * php /var/www/html/cli/process_hotfolder.php >> /var/log/hotfolder/processing.log 2>&1

# Archive old processed files monthly (1st of month, 3 AM)
# Moves files older than 30 days to archive folder
0 3 1 * * find /var/www/hotfolder/processed -name "*.csv" -mtime +30 -exec mv {} /var/www/hotfolder/archive/ \; >> /var/log/hotfolder/archive.log 2>&1

# Clean error logs older than 90 days
# Prevents disk space issues from old error files
0 4 1 * * find /var/www/hotfolder/errors -name "*.log" -mtime +90 -delete >> /var/log/hotfolder/cleanup.log 2>&1

# Environment variable (needed for PHP to find commands)
PATH=/usr/local/bin:/usr/bin:/bin

# Empty line required at end of crontab file
```

**Why every 15 minutes:**
- Report arrives once per day (not real-time needed)
- 15-min delay acceptable (vs. daily check)
- Frequent enough to catch manual uploads too
- Not so frequent as to waste resources
- Standard cron interval (easy to remember)

**Can adjust frequency:**
```cron
# More frequent (every 5 minutes):
*/5 * * * * ...

# Less frequent (hourly):
0 * * * * ...

# Daily at 6 AM only:
0 6 * * * ...
```

#### Step 3.2: Rebuild Docker Image

```bash
# Rebuild with cron
docker compose build web

# Restart to apply changes
docker compose up -d

# Verify cron is running
docker exec circulation_web ps aux | grep cron

# Expected:
# root     123  cron
```

#### Step 3.3: Test Cron Execution

**Method 1: Wait for next run (max 15 min)**
```bash
# Drop test file
echo "test" > hotfolder/incoming/AllSubscriberReport_test.csv

# Wait for cron to run (check every few minutes)
watch -n 60 'ls -lh hotfolder/incoming/ hotfolder/processed/'

# After cron runs, file should move from incoming â†’ processed
```

**Method 2: Run manually to test immediately**
```bash
# Execute inside container
docker exec circulation_web php /var/www/html/cli/process_hotfolder.php

# Check logs
docker exec circulation_web cat /var/log/hotfolder/processing.log
```

**Method 3: Trigger cron manually (for testing)**
```bash
# Force cron to run immediately (development only)
docker exec circulation_web cron -f
```

---

### Phase 4: Production Deployment

**Objective:** Deploy SFTP automation to production Synology NAS.

#### Step 4.1: Update Production Configuration

**File:** `docker-compose.prod.yml`

```yaml
services:
  # ... existing services ...

  # SFTP Server (Production)
  sftp:
    image: atmoz/sftp:latest
    container_name: circulation_sftp
    restart: unless-stopped

    ports:
      - "2222:22"

    networks:
      - circulation_network

    volumes:
      - ./hotfolder/incoming:/home/newzware/incoming

    environment:
      # Use environment variable for password (stored in .env)
      SFTP_USERS: "newzware:${SFTP_PASSWORD}:1000:1000:incoming"

    # Security: Resource limits (prevent DoS)
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M

    # Security: Drop all capabilities
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - FOWNER

    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep -v grep | grep sshd"]
      interval: 30s
      timeout: 5s
      retries: 3
```

**Create production .env file (on NAS):**

```bash
# SSH to NAS
ssh it@192.168.1.254
cd /volume1/docker/nwdownloads

# Create .env with strong password
cat > .env << 'EOF'
# SFTP Password (rotate every 90 days)
# Generated: 2025-12-06
# Next rotation: 2026-03-06
SFTP_PASSWORD=Xk9$mP2#vL8@qR4!nF7%wB3^jH6&sD1

# Database credentials (existing)
DB_HOST=database
DB_NAME=circulation_dashboard
DB_USER=root
DB_PASSWORD=RootPassword456!
EOF

# Secure permissions
chmod 600 .env
chown root:root .env
```

**Generate strong password:**
```bash
# On local machine
openssl rand -base64 32 | tr -d "=+/" | cut -c1-32

# Or use password manager
# Requirements: 20+ chars, mixed case, numbers, symbols
```

#### Step 4.2: Deploy to Production

```bash
# On local machine: Build and push updated image
cd $PROJECT_ROOT
./build-and-push.sh

# SSH to production NAS
ssh it@192.168.1.254
cd /volume1/docker/nwdownloads

# Pull latest images
sudo docker compose -f docker-compose.prod.yml pull

# Stop current containers
sudo docker compose -f docker-compose.prod.yml down

# Start with new configuration
sudo docker compose -f docker-compose.prod.yml up -d

# Verify all containers running
sudo docker compose -f docker-compose.prod.yml ps

# Expected:
# circulation_web    Up (healthy)
# circulation_db     Up (healthy)
# circulation_sftp   Up (healthy)  0.0.0.0:2222->22/tcp

# Check SFTP logs
sudo docker logs circulation_sftp --tail 50
```

#### Step 4.3: Configure Ubiquiti Port Forwarding

**UniFi Network Controller:**

1. **Login** â†’ UniFi Controller (https://unifi.ui.com or local IP)

2. **Settings** â†’ **Routing & Firewall** â†’ **Port Forwarding**

3. Click **Create Entry**

4. Configure:
   ```
   Name: Circulation Dashboard SFTP
   Enabled: âœ“

   From: Internet (WAN)
   Port: 2222
   Forward IP: 192.168.1.254
   Forward Port: 2222
   Protocol: TCP

   Log: âœ“ Enabled (for monitoring)
   ```

5. **Optional: IP Whitelist** (if Newzware has static IP)
   ```
   Settings â†’ Firewall â†’ Rules â†’ WAN IN

   Create Rule:
     Name: Allow SFTP from Newzware
     Action: Accept
     Source: [Newzware Public IP]
     Destination: 192.168.1.254:2222
     Protocol: TCP

   Create Rule:
     Name: Block Other SFTP
     Action: Drop
     Source: Any
     Destination: 192.168.1.254:2222
     Protocol: TCP
     Log: âœ“
   ```

6. **Apply Changes**

**Verify port forwarding:**
```bash
# From external network (use phone hotspot)
sftp -P 2222 newzware@cdash.upstatetoday.com
# or
sftp -P 2222 newzware@YOUR_PUBLIC_IP

# Should connect successfully
```

---

### Phase 5: Newzware Configuration

**Objective:** Configure Newzware to automatically export reports via SFTP.

#### Step 5.1: Generate SSH Key Pair (Recommended Method)

**On Newzware Server (Windows):**

```bash
# Open Command Prompt or PowerShell
# Navigate to Newzware config directory
cd C:\Newzware\config\sftp

# Generate SSH key pair (no passphrase for automation)
ssh-keygen -t rsa -b 4096 -f newzware_sftp_key -C "newzware-sftp-automation" -N ""

# Result:
# newzware_sftp_key      (private key - keep secure)
# newzware_sftp_key.pub  (public key - upload to SFTP server)
```

**Important:**
- **Private key** stays on Newzware server (never share!)
- **Public key** gets added to SFTP server's authorized_keys
- No passphrase needed (automation requirement)

#### Step 5.2: Add Public Key to SFTP Server

**On Development Machine:**

```bash
# Read public key content
cat C:\Newzware\config\sftp\newzware_sftp_key.pub

# Output will be like:
# ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC... newzware-sftp-automation
```

**Add to SFTP Container:**

**Method 1: Via docker exec (Easiest)**
```bash
# SSH to NAS
ssh it@192.168.1.254

# Add public key to authorized_keys
sudo docker exec circulation_sftp sh -c "mkdir -p /home/newzware/.ssh && echo 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC...' >> /home/newzware/.ssh/authorized_keys"

# Set correct permissions
sudo docker exec circulation_sftp sh -c "chmod 700 /home/newzware/.ssh && chmod 600 /home/newzware/.ssh/authorized_keys"
```

**Method 2: Via volume mount (Persistent)**
- Add volume mount in docker-compose.yml:
  ```yaml
  volumes:
    - ./sftp/authorized_keys:/home/newzware/.ssh/authorized_keys:ro
  ```
- Create `./sftp/authorized_keys` file with public key
- Restart container

#### Step 5.3: Configure Newzware Report Scheduler

**In Newzware:**

1. **Navigate to Report Scheduler**
   - Reports â†’ Report Macro Automatic Scheduler
   - Or: Tools â†’ Scheduled Tasks â†’ Reports

2. **Create New Schedule or Edit Existing**
   - Report: **All Subscriber Report**
   - Output Format: **CSV** or **PLAIN TEXT**

3. **Set Schedule**
   - Frequency: **Daily**
   - Time: **5:00 AM** (before business hours)
   - Days: **Monday - Sunday** (runs every day)

4. **Configure Output Script**

   **Field:** "Output Script"

   **Value (SSH Key - Recommended):**
   ```
   com.icanon.report.DefaultSchedulerScript,protocol=sftp;server=cdash.upstatetoday.com;port=2222;user=newzware;privatekey=C:/Newzware/config/sftp/newzware_sftp_key;folder=/incoming;outputfile=AllSubscriberReport_<datestamp>.csv;skipempty=true;
   ```

   **Alternative Value (Password Fallback):**
   ```
   com.icanon.report.DefaultSchedulerScript,protocol=sftp;server=cdash.upstatetoday.com;port=2222;user=newzware;password=Xk9$mP2#vL8@qR4!nF7%wB3^jH6&sD1;folder=/incoming;outputfile=AllSubscriberReport_<datestamp>.csv;skipempty=true;
   ```

   **Important Notes:**
   - **No line breaks** - entire script on one line
   - **No spaces** around semicolons or equals signs
   - `<datestamp>` automatically replaced with YYYYMMDD (e.g., 20251206)
   - Result filename: `AllSubscriberReport_20251206.csv`

5. **Enable Email Notifications (Optional)**
   - Configure `jftpemail.properties` file on Newzware server
   - Emails sent on success/failure
   - See Newzware documentation for email setup

**Why 5:00 AM:**
- Before business hours (6 AM - 5 PM)
- Allows time for processing before users arrive
- Off-peak network hours (faster transfers)
- Consistent with other automated tasks
- Data ready by 6 AM when cron processes it

**Configuration Breakdown:**

| Parameter | Value | Purpose |
|-----------|-------|---------|
| `protocol` | `sftp` | Use secure SFTP (not FTP) |
| `server` | `cdash.upstatetoday.com` | Production domain (or IP for testing) |
| `port` | `2222` | Non-standard port (security) |
| `user` | `newzware` | SFTP username |
| `privatekey` | `C:/Newzware/config/sftp/newzware_sftp_key` | SSH private key path |
| `folder` | `/incoming` | Target directory |
| `outputfile` | `AllSubscriberReport_<datestamp>.csv` | Filename with auto-date |
| `skipempty` | `true` | Don't upload zero-byte files |

#### Step 5.4: Test Manual Export

**In Newzware:**
1. Find **All Subscriber Report** in Report Scheduler
2. Click **Test Report Schedule** or **Run Now**
3. Watch for completion (should take 30-60 seconds)
4. Check for success/error messages in Newzware logs

**Verify on server:**
```bash
# SSH to NAS
ssh it@192.168.1.254

# Check incoming folder
ls -lh /volume1/docker/nwdownloads/hotfolder/incoming/

# Should see new CSV file within 1-2 minutes
# Example: AllSubscriberReport_20251206_140530.csv

# Wait 15 minutes (or trigger cron manually)
# File should move to processed folder

ls -lh /volume1/docker/nwdownloads/hotfolder/processed/

# Should see: AllSubscriberReport_20251206_140530_20251206_141500.csv
#             (original filename + processing timestamp)
```

#### Step 5.5: Verify Database Updated

**Check dashboard:**
```
Visit: https://cdash.upstatetoday.com

Dashboard should show updated data from CSV import
```

**Check database directly:**
```bash
# On NAS
sudo docker exec circulation_db mariadb -uroot -p'RootPassword456!' \
  -D circulation_dashboard \
  -e "SELECT MAX(snapshot_date), COUNT(*) FROM daily_snapshots;"

# Should show today's date and record count
```

---

## Monitoring & Maintenance

### Daily Monitoring

**What to check:**

**1. SFTP Access Logs**
```bash
# View recent logins
ssh it@192.168.1.254
sudo docker logs circulation_sftp --since 24h | grep -i "Accepted\|Failed"

# Expected (daily):
# [timestamp] Accepted password for newzware from X.X.X.X
#
# If you see:
# [timestamp] Failed password for newzware from X.X.X.X
# â†’ Investigate: Wrong password? Brute force attempt?
```

**2. Processing Logs**
```bash
# View processing results
sudo docker exec circulation_web cat /var/log/hotfolder/processing.log | tail -50

# Look for:
# âœ“ [SUCCESS] messages (good)
# âœ— [ERROR] messages (investigate)
# âš  [WARNING] messages (review)
```

**3. File Status**
```bash
# Check folder contents
ls -lh /volume1/docker/nwdownloads/hotfolder/incoming/
  # Should be empty (files processed and moved)
  # If files stuck here: Check processing logs for errors

ls -lh /volume1/docker/nwdownloads/hotfolder/processed/ | tail -10
  # Should show recent successful imports
  # Verify timestamps are recent (last 24 hours)

ls -lh /volume1/docker/nwdownloads/hotfolder/errors/
  # Should be empty (no errors)
  # If files here: Review .log files for details
```

**4. Dashboard Health**
```bash
# Visit dashboard
https://cdash.upstatetoday.com

# Verify:
# - Date shows today (or latest upload date)
# - Subscriber counts reasonable
# - Trends look correct
```

### Weekly Monitoring

**1. Review Error Patterns**
```bash
# Analyze error logs
cd /volume1/docker/nwdownloads/hotfolder/errors

# Count errors by type
grep -h "ERROR:" *.log | sort | uniq -c

# Common errors to investigate:
# - "Database connection failed" â†’ Check DB health
# - "Invalid CSV format" â†’ Check Newzware export settings
# - "Permission denied" â†’ Check folder permissions
```

**2. Disk Space Check**
```bash
# Check hotfolder size
du -sh /volume1/docker/nwdownloads/hotfolder/*

# Expected:
# incoming/   - Small (< 10 MB, files processed quickly)
# processed/  - Growing (old imports accumulate)
# errors/     - Small (hopefully empty)
# archive/    - Large (long-term storage)

# If processed/ > 1 GB: Run archive job manually
# find hotfolder/processed -name "*.csv" -mtime +30 -exec mv {} hotfolder/archive/ \;
```

**3. Security Audit**
```bash
# Check for suspicious activity
sudo docker logs circulation_sftp | grep -i "Failed password" | wc -l

# If > 100: Possible brute force attack
# Action: Review source IPs, consider IP whitelist
```

### Monthly Maintenance

**1. Archive Old Files**
```bash
# Move processed files > 30 days to archive
find /volume1/docker/nwdownloads/hotfolder/processed \
  -name "*.csv" -mtime +30 \
  -exec mv {} /volume1/docker/nwdownloads/hotfolder/archive/ \;

# Compress archived files > 90 days
find /volume1/docker/nwdownloads/hotfolder/archive \
  -name "*.csv" -mtime +90 \
  -exec gzip {} \;
```

**2. Review and Clean Error Files**
```bash
# Review errors from last month
cd /volume1/docker/nwdownloads/hotfolder/errors
ls -lh

# For each error:
# 1. Read .log file to understand issue
# 2. Fix root cause (Newzware config, CSV format, etc.)
# 3. Delete error files after resolution

# Delete errors > 90 days (already investigated)
find . -name "*.log" -mtime +90 -delete
find . -name "*.csv*" -mtime +90 -delete
```

**3. Rotate SFTP Password**
```bash
# Every 90 days, rotate password

# Generate new password
openssl rand -base64 32 | tr -d "=+/" | cut -c1-32

# Update .env file
ssh it@192.168.1.254
nano /volume1/docker/nwdownloads/.env
# Update SFTP_PASSWORD=new_password_here

# Restart SFTP container
cd /volume1/docker/nwdownloads
sudo docker compose -f docker-compose.prod.yml restart sftp

# Update Newzware SFTP settings with new password
# Test with manual export
```

### Alerting Setup (Optional)

**Email Alerts on Failure:**

**Create:** `web/cli/alert_on_failure.sh`

```bash
#!/bin/bash
# Check for processing failures and send email

LOG_FILE="/var/log/hotfolder/processing.log"
ERROR_COUNT=$(grep -c "\[ERROR\]" "$LOG_FILE" 2>/dev/null || echo 0)

if [ "$ERROR_COUNT" -gt 0 ]; then
    # Get last 10 errors
    ERRORS=$(grep "\[ERROR\]" "$LOG_FILE" | tail -10)

    # Send email
    echo "Hotfolder processing errors detected:

$ERRORS

Please investigate: https://cdash.upstatetoday.com/hotfolder/errors/
" | mail -s "Circulation Dashboard: Import Errors" admin@upstatetoday.com
fi
```

**Add to cron:**
```cron
# Check for errors hourly
0 * * * * /var/www/html/cli/alert_on_failure.sh
```

---

## Troubleshooting

### Common Issues

#### Issue 1: Files Not Arriving from Newzware

**Symptoms:**
- Newzware shows "Export Successful"
- No files in hotfolder/incoming/
- SFTP logs show no connection

**Diagnosis:**
```bash
# Test SFTP from external network
sftp -P 2222 newzware@cdash.upstatetoday.com
# Does this work?

# Check port forwarding
sudo docker exec circulation_sftp netstat -tuln | grep :22
# Is port 22 listening inside container?

# Check firewall
sudo docker logs circulation_sftp | grep -i "connection"
# Any refused connections?
```

**Solutions:**

**A. Port Forwarding Not Working**
```
1. Check Ubiquiti settings
   - Is rule enabled?
   - Correct forward IP (192.168.1.254)?
   - Correct port (2222)?

2. Check if port is open externally
   - Visit: https://www.yougetsignal.com/tools/open-ports/
   - Enter your public IP and port 2222
   - Should show "Open"

3. Check NAS firewall
   - DSM â†’ Control Panel â†’ Security â†’ Firewall
   - Ensure port 2222 allowed
```

**B. Newzware Can't Connect**
```
1. Verify Newzware settings
   - Correct hostname/IP?
   - Correct port (2222, not 22)?
   - Correct username (newzware)?
   - Correct password?

2. Test from Newzware server directly
   - SSH to Newzware server
   - Try: sftp -P 2222 newzware@cdash.upstatetoday.com
   - Does it work?

3. Check Newzware error logs
   - Look for connection errors
   - Common: "Connection refused", "Permission denied"
```

#### Issue 2: Files Arrive But Not Processed

**Symptoms:**
- Files in hotfolder/incoming/
- Not moved to processed/
- No cron execution logs

**Diagnosis:**
```bash
# Check if cron is running
docker exec circulation_web ps aux | grep cron
# Should show: root ... cron

# Check cron logs
docker exec circulation_web cat /var/log/cron.log
# Any errors?

# Run processor manually
docker exec circulation_web php /var/www/html/cli/process_hotfolder.php
# Does it work manually?
```

**Solutions:**

**A. Cron Not Running**
```bash
# Restart container
docker compose restart web

# Verify cron started
docker exec circulation_web ps aux | grep cron
```

**B. Script Has Errors**
```bash
# Check PHP syntax
docker exec circulation_web php -l /var/www/html/cli/process_hotfolder.php
# Should show: "No syntax errors"

# Check for exceptions
docker exec circulation_web cat /var/log/hotfolder/processing.log | grep -i "exception"
```

**C. Permission Issues**
```bash
# Check file ownership
docker exec circulation_web ls -lh /var/www/hotfolder/incoming/
# Should be readable by www-data user

# Fix permissions if needed
docker exec circulation_web chown -R www-data:www-data /var/www/hotfolder/
```

#### Issue 3: Processing Fails with Database Errors

**Symptoms:**
- Files moved to errors/
- Logs show "Database connection failed" or "SQL error"

**Diagnosis:**
```bash
# Check database is running
docker compose ps
# Is circulation_db status "Up (healthy)"?

# Test database connection
docker exec circulation_db mariadb -uroot -p'RootPassword456!' -e "SELECT 1;"
# Should show: 1

# Check for SQL errors in logs
docker exec circulation_web cat /var/log/hotfolder/processing.log | grep -i "sql"
```

**Solutions:**

**A. Database Not Running**
```bash
# Restart database
docker compose restart database

# Wait for healthy status
docker compose ps
```

**B. Wrong Credentials**
```bash
# Verify environment variables
docker exec circulation_web env | grep DB_
# Should show correct credentials

# Update docker-compose.yml if wrong
# Restart: docker compose up -d
```

**C. Table Schema Changed**
```bash
# Check if required tables exist
docker exec circulation_db mariadb -uroot -p'RootPassword456!' \
  -D circulation_dashboard \
  -e "SHOW TABLES;"

# Should show:
# - daily_snapshots
# - subscriber_snapshots

# If missing: Run migration scripts
```

#### Issue 4: SFTP Connection Slow or Drops

**Symptoms:**
- Newzware export takes long time
- Partial file uploads
- Connection timeouts

**Diagnosis:**
```bash
# Check container resources
docker stats circulation_sftp

# Is CPU or memory maxed out?
# Is network TX/RX saturated?
```

**Solutions:**

**A. Resource Limits Too Low**
```yaml
# In docker-compose.prod.yml:
deploy:
  resources:
    limits:
      cpus: '1.0'      # Increase from 0.5
      memory: 512M     # Increase from 256M
```

**B. Network Issues**
```bash
# Test bandwidth to NAS
iperf3 -c 192.168.1.254

# If slow: Check for:
# - Network congestion
# - Ubiquiti QoS settings
# - Cable issues
```

**C. Large File Size**
```bash
# Check file size
ls -lh hotfolder/incoming/

# If > 50 MB: Increase timeout in Newzware
# Or split export into smaller files
```

---

## Future Enhancements

### Phase 6: Additional Report Types

**Vacation Holds Report:**
```php
// In ImportProcessor.php
public function processVacationReport($filePath) {
    // Process vacation holds data
    // Update vacation_snapshots table
    // Track when holds started/ended
}
```

**Expiring Subscriptions Report:**
```php
public function processExpiringReport($filePath) {
    // Identify subscriptions expiring soon
    // Generate renewal reminders
    // Update expiration forecasts
}
```

**Configuration:**
```yaml
# In Newzware:
Vacation Report:
  Filename: VacationReport_%Y%m%d.csv
  Schedule: Daily 5:15 AM

Expiring Report:
  Filename: ExpiringReport_%Y%m%d.csv
  Schedule: Daily 5:30 AM
```

### Phase 7: Real-Time Processing

**File Watcher (inotify):**
```php
// watch_hotfolder.php
<?php
// Use inotify to watch for file changes
$inotify = inotify_init();
$watch = inotify_add_watch($inotify, '/var/www/hotfolder/incoming', IN_CLOSE_WRITE);

while (true) {
    $events = inotify_read($inotify);
    foreach ($events as $event) {
        if ($event['mask'] & IN_CLOSE_WRITE) {
            // File fully written, process immediately
            processFile($event['name']);
        }
    }
}
?>
```

**When to implement:**
- If reports need processing within minutes (not 15-min delay)
- If multiple reports arrive throughout day
- If real-time dashboard updates required

### Phase 8: Advanced Security

**Fail2Ban Integration:**
```yaml
# docker-compose.prod.yml
services:
  fail2ban:
    image: crazymax/fail2ban:latest
    container_name: circulation_fail2ban
    volumes:
      - ./fail2ban:/data
      - /var/log:/var/log:ro
    environment:
      - SSHD_LOG=/var/log/circulation_sftp.log
      - BANTIME=3600
      - FINDTIME=600
      - MAXRETRY=5
```

**SSH Key Authentication:**
```yaml
# If Newzware adds key support
sftp:
  volumes:
    - ./sftp/authorized_keys:/home/newzware/.ssh/keys:ro
  environment:
    SFTP_USERS: "newzware::1000:1000:incoming"
    # Empty password = key-only auth
```

**2FA/MFA:**
- Google Authenticator for admin access
- Duo Security integration
- Hardware keys (YubiKey)

### Phase 9: Monitoring Dashboard

**Grafana + Prometheus:**
- Visualize import metrics
- Alert on failures
- Track processing times
- Monitor SFTP connections

**Metrics to track:**
- Files processed per day
- Processing success rate
- Average processing time
- Disk space usage
- SFTP connection attempts
- Failed logins (security)

---

## Senior Developer Notes

### Architectural Decisions

**Why Docker for SFTP (Not Standalone Service)?**

**Considered:**
1. Synology built-in SFTP
2. Standalone SFTP daemon
3. Docker container (chosen)

**Decision Matrix:**

| Factor | Synology SFTP | Standalone | Docker |
|--------|---------------|------------|--------|
| Isolation | âŒ Full access | âš ï¸ Chroot jail | âœ… Container |
| Portability | âŒ NAS-specific | âš ï¸ OS-specific | âœ… Any platform |
| Version Control | âŒ GUI config | âš ï¸ Manual files | âœ… docker-compose.yml |
| Easy Rollback | âŒ Difficult | âš ï¸ Manual | âœ… One command |
| Team Knowledge | âš ï¸ Synology-specific | âš ï¸ Server admin | âœ… Docker standard |

**Result:** Docker wins on isolation, portability, and team knowledge.

**Trade-off:** Slight overhead (container resources), acceptable for benefits.

### Design Patterns Applied

**1. Single Responsibility Principle (SRP)**
```
ImportProcessor class:
  âœ“ Only processes CSV files
  âœ— Doesn't handle file upload
  âœ— Doesn't handle web requests
  âœ— Doesn't handle cron scheduling

Result: Reusable, testable, maintainable
```

**2. Separation of Concerns**
```
Layer 1 (Transport):     SFTP container
Layer 2 (Storage):       Hotfolder
Layer 3 (Processing):    ImportProcessor
Layer 4 (Persistence):   Database
Layer 5 (Presentation):  Dashboard

Each layer independent, replaceable
```

**3. Fail-Safe Defaults**
```
Default behavior: Move to errors folder (not delete)
Why: Preserve data for debugging
      Can always delete manually
      Can't un-delete
```

**4. Idempotency**
```
Running process_hotfolder.php multiple times:
  - Same file processed once (moved after success)
  - Database UPSERT (safe to re-run)
  - No duplicate records
  - No side effects

Result: Safe to retry, cron overlap doesn't break things
```

### Security Philosophy

**Zero Trust Model:**
```
1. SFTP user trusts nothing
   - Can't see NAS filesystem
   - Can't execute commands
   - Only write to one folder

2. Processing script trusts nothing
   - Validates file size
   - Validates CSV format
   - Sanitizes input data
   - Prepared statements (SQL injection proof)

3. Network trusts nothing
   - Firewall drops unknown IPs
   - Port forwarding on non-standard port
   - All access logged

Result: Compromise of one layer doesn't cascade
```

**Defense in Depth:**
```
Layers:
1. Firewall (Ubiquiti)
2. Port forwarding (non-standard port)
3. Docker isolation
4. Filesystem restrictions (chroot)
5. Strong authentication
6. Input validation
7. Monitoring/logging

Analogy: Multiple locked doors
         Burglar must pick all locks
         Each lock makes attack harder
```

### Performance Considerations

**Why 15-Minute Processing Interval?**

**Analysis:**
```
Report frequency: 1x per day (5 AM)
Processing time: ~2-5 seconds (8000 rows)
Acceptable delay: 15 minutes (data not time-critical)

Options:
  A) Real-time (file watcher): Complex, unnecessary
  B) Hourly: Too slow (60-min delay)
  C) 15 minutes: Sweet spot
  D) 5 minutes: More load, no benefit

Choice: C (15 minutes)
  - Simple cron
  - Acceptable delay
  - Low resource usage
  - Catches manual uploads too
```

**Database Performance:**

**UPSERT Strategy:**
```sql
-- Fast path: Record exists
INSERT INTO subscriber_snapshots ...
ON DUPLICATE KEY UPDATE ...

-- vs. Slow path: Check then insert/update
SELECT * FROM subscriber_snapshots WHERE ...;
IF exists THEN
  UPDATE ...;
ELSE
  INSERT ...;
END IF;

UPSERT = One query (not two)
Result: 2x faster
```

**Bulk Insert Optimization:**
```php
// Slow: 8000 individual INSERTs
foreach ($rows as $row) {
    $stmt->execute($row);  // 8000 round-trips to DB
}

// Fast: Batch INSERT
$values = [];
foreach ($rows as $row) {
    $values[] = $row;  // Build array
}
INSERT INTO table VALUES (row1), (row2), ... (row8000);
// 1 round-trip to DB

Result: 100x faster for large imports
```

### Scalability Path

**Current Capacity:**
```
Reports: 1 per day (AllSubscriber)
Size: ~8000 rows, 2 MB
Processing: 2-5 seconds
Bottleneck: None (plenty of headroom)
```

**Scale to 10x:**
```
Reports: 10 per day (multiple types)
Size: 80,000 rows total
Processing: 20-50 seconds
Changes needed: None (still well within limits)
```

**Scale to 100x:**
```
Reports: 100 per day
Size: 800,000 rows
Processing: 200-500 seconds (3-8 minutes)
Changes needed:
  - Increase cron frequency (every 5 min)
  - Add database indexes
  - Optimize bulk insert size
  - Consider queue system
```

**When to refactor:**
- If processing > 10 minutes (blocking cron)
- If disk space > 10 GB (archive strategy)
- If multiple clients (need queue)
- If real-time updates needed (file watcher)

### Testing Strategy

**Unit Tests (Future):**
```php
// test/ImportProcessorTest.php
class ImportProcessorTest extends PHPUnit\Framework\TestCase {
    public function testProcessValidCSV() {
        $processor = new ImportProcessor($mockPDO);
        $result = $processor->processAllSubscriberReport('test.csv');
        $this->assertTrue($result['success']);
    }

    public function testRejectsInvalidCSV() {
        // Test malformed CSV
        // Test wrong columns
        // Test SQL injection attempts
    }
}
```

**Integration Tests:**
```bash
# test/integration_test.sh

# 1. Upload test CSV via SFTP
echo "Test 1: SFTP Upload"
sftp -P 2222 newzware@localhost <<EOF
put test_data/AllSubscriberReport.csv incoming/
exit
EOF

# 2. Wait for processing
sleep 20

# 3. Verify file moved to processed
test -f hotfolder/processed/AllSubscriberReport_*.csv
echo "âœ“ Test 1 Passed"

# 4. Verify database updated
DB_COUNT=$(docker exec circulation_db mariadb ... -e "SELECT COUNT(*) ...")
test "$DB_COUNT" -gt 0
echo "âœ“ Test 2 Passed"
```

**Load Tests:**
```bash
# Simulate 100 concurrent uploads
for i in {1..100}; do
    sftp -P 2222 newzware@localhost <<EOF &
    put test_data/report_$i.csv incoming/
    exit
EOF
done

wait

# Verify all processed
count=$(ls hotfolder/processed/*.csv | wc -l)
test $count -eq 100
```

### Operational Excellence

**Observability (The Three Pillars):**

**1. Logs:**
```
Where:
  - SFTP access: docker logs circulation_sftp
  - Processing: /var/log/hotfolder/processing.log
  - Errors: hotfolder/errors/*.log

What to log:
  - Every file upload (timestamp, size, user)
  - Processing start/end (file, duration, result)
  - Errors (with stack trace)
  - Performance metrics (rows/sec)
```

**2. Metrics:**
```
Track:
  - Files processed (count)
  - Processing success rate (%)
  - Average processing time (seconds)
  - Disk space used (GB)
  - SFTP connections (count)

Store:
  - Time-series database (Prometheus)
  - Or simple CSV (for now)
```

**3. Tracing:**
```
Follow request:
  Newzware â†’ SFTP â†’ Hotfolder â†’ Processor â†’ Database

Tag each step with:
  - Request ID (filename)
  - Timestamp
  - Duration

Result: Can trace any file through entire pipeline
```

**Runbook for On-Call:**
```
If paged: "Import Failed"

1. Check error folder
   ls hotfolder/errors/

2. Read latest error log
   cat hotfolder/errors/latest.log

3. Common issues:
   - Database down â†’ Restart DB container
   - Invalid CSV â†’ Check Newzware config
   - Disk full â†’ Archive old files

4. Escalate if:
   - Unknown error
   - Database corruption
   - Security incident
```

### Code Review Checklist

**Before merging:**
- [ ] SFTP isolation verified (can't access NAS)
- [ ] Strong password in .env (not hardcoded)
- [ ] Input validation present (CSV, file size)
- [ ] SQL uses prepared statements (no injection)
- [ ] Error handling (try/catch, graceful failure)
- [ ] Logging sufficient (can debug issues)
- [ ] Files preserved (moved, not deleted)
- [ ] Documentation complete (this file)
- [ ] Tested locally (SFTP, processing, cron)
- [ ] Tested on staging (if available)
- [ ] Rollback plan documented

---

## Conclusion

**What We Built:**
- Automated SFTP import pipeline
- Zero manual intervention
- Secure, isolated, auditable
- Production-ready architecture

**Key Benefits:**
- Time savings: 15 min/week â†’ 0 min/week
- Reliability: Never forget to update
- Scalability: Ready for multiple reports
- Security: Defense in depth

**Success Metrics:**
- Files processed: 100%
- Processing time: < 10 seconds
- Uptime: 99.9%
- Security incidents: 0

**Total Implementation Time:** 2-3 hours
**Return on Investment:** Immediate (time savings + reliability)

**Next Steps:**
1. Implement Phase 1-5 (core functionality)
2. Monitor for 1 week (verify stability)
3. Add Phase 6+ (additional features)

---

**Document Version:** 1.0
**Last Updated:** 2025-12-06
**Maintained By:** Development Team
**Review Schedule:** Quarterly

**Related Documentation:**
- `SECURITY-IMPROVEMENTS-TODO.md` - Security hardening
- `PRODUCTION-CHECKLIST.md` - Deployment procedures
- `DEPLOY-SECURITY-UPDATES.md` - Monday deployment guide

---

*"Automate the boring stuff so you can focus on what matters."*

*This document represents production-grade thinking: secure, scalable, maintainable, and documented for the long term.*
